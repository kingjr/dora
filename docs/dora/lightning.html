<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dora.lightning API documentation</title>
<meta name="description" content="Support for PyTorch lightning. You should just replace the call
to `Trainer(...)` with `get_trainer(...)`." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dora.lightning</code></h1>
</header>
<section id="section-intro">
<p>Support for PyTorch lightning. You should just replace the call
to <code>Trainer(&hellip;)</code> with <code><a title="dora.lightning.get_trainer" href="#dora.lightning.get_trainer">get_trainer()</a>(&hellip;)</code>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

&#34;&#34;&#34;
Support for PyTorch lightning. You should just replace the call
to `Trainer(...)` with `get_trainer(...)`.
&#34;&#34;&#34;
import argparse
import functools
import inspect
import os
import typing as tp

from pytorch_lightning.callbacks import Callback
from pytorch_lightning.loggers import LightningLoggerBase, TensorBoardLogger
from pytorch_lightning.plugins.environments import ClusterEnvironment
from pytorch_lightning.plugins import TrainingTypePluginsRegistry
from pytorch_lightning.plugins.training_type.ddp import DDPPlugin
from pytorch_lightning.trainer import Trainer
from pytorch_lightning.utilities.argparse import from_argparse_args
import torch

from . import distrib
from .xp import get_xp, is_xp


class DoraEnvironment(ClusterEnvironment):
    def __init__(self):
        super().__init__()
        self.spec = distrib.get_distrib_spec()

    def creates_children(self) -&gt; bool:
        return True

    def creates_processes_externally(self) -&gt; bool:
        return True

    def master_address(self) -&gt; str:
        return &#34;&#34;

    def master_port(self) -&gt; int:
        assert False

    def world_size(self) -&gt; int:
        return self.spec.world_size

    def set_world_size(self, size: int) -&gt; None:
        pass

    def global_rank(self) -&gt; int:
        return self.spec.rank

    def set_global_rank(self, rank: int) -&gt; None:
        pass

    def local_rank(self) -&gt; int:
        return self.spec.local_rank

    def node_rank(self) -&gt; int:
        return self.spec.node_rank


@TrainingTypePluginsRegistry.register(&#34;dora_ddp&#34;)
class DoraDDPPlugin(DDPPlugin):
    &#34;&#34;&#34;DDP plugin for compatibility with Dora.
    &#34;&#34;&#34;
    def init_ddp_connection(self, global_rank: tp.Optional[int] = None,
                            world_size: tp.Optional[int] = None) -&gt; None:
        distrib.init(self.torch_distributed_backend)


class RestoreDoraHistory(Callback):
    &#34;&#34;&#34;Make sure Dora history, and checkpoint state are in sync.
    &#34;&#34;&#34;
    def __init__(self):
        self.link = get_xp().link

    def on_load_checkpoint(self, trainer, pl_module, checkpoint):
        history = checkpoint[&#39;dora_link_history&#39;]
        self.link.update_history(history)

    def on_save_checkpoint(self, trainer, pl_module, checkpoint):
        checkpoint[&#39;dora_link_history&#39;] = self.link.history
        return checkpoint


class _ArmDoraLogger(Callback):
    # Some metrics are per step, some per epoch, I want only the per epoch.
    # At the moment this is not supported by PL, so I manually trigger the logger
    # save on some events that are registered with this callback.
    def __init__(self, logger):
        super().__init__()
        self.logger = logger
        self._first = True

    def on_train_epoch_start(self, trainer, pl_module):
        if self._first:
            self._first = False
            return
        self.logger._push()

    def on_train_end(self, trainer, pl_module):
        self.logger._push()

    def on_test_end(self, trainer, pl_module):
        self.logger._repush()


class _DummySLURMConnector:
    # Deactivate SLURM connector because Submitit does it already,
    # and this can cost us an unfinished epoch, which we don&#39;t want!!
    def register_slurm_signal_handlers(self):
        pass


class DoraHistoryLogger(LightningLoggerBase):
    def __init__(self):
        super().__init__()
        self.link = get_xp().link
        self.folder = get_xp().folder
        self._metrics = {}

    def log_metrics(self, metrics, step):
        self._metrics.update(metrics)

    def _push(self):
        self.link.push_metrics(self._metrics)
        self._metrics = {}

    def _repush(self):
        history = self.link.history
        history[-1].update(self._metrics)
        self.link.update_history(history)
        self._metrics = {}

    @property
    def save_dir(self):
        return self.folder

    @property
    def name(self):
        return &#34;DoraHistoryLogger&#34;

    def experiment(self) -&gt; tp.Any:
        &#34;&#34;&#34;Return the experiment object associated with this logger.&#34;&#34;&#34;
        pass

    def log_hyperparams(self, params: argparse.Namespace, *args, **kwargs):
        pass

    @property
    def version(self) -&gt; int:
        &#34;&#34;&#34;Return the experiment version.&#34;&#34;&#34;
        return 0


def get_trainer(*args, add_dora_logger=True, no_unfinished_epochs=True, **kwargs):
    &#34;&#34;&#34;Return a PL trainer, adding the necessary glue code to make everything works.
    The arguments are exactly the same as for `pytorch_lightning.trainer.Trainer`,
    with a few extras documented after.

    ..note:: You should not pass `gpus=` or `num_nodes=` arguments as those will be filled by Dora.

    Args:
        add_dora_logger (bool): if True, adds a Dora Logger to automatically
            forward the metrics (those logged with per_epoch=True), otherwise
            pushing metrics will be up to you.
        no_unfinished_epochs (bool): if True, deactivates SLURM signal handling
            by PL, which can result in half finished epoch with each interruption.
            It is recommended to instead dump a checkpoint every epoch and resume
            from that one so that training is reliable.

    &#34;&#34;&#34;
    if not is_xp():
        raise RuntimeError(&#34;This can only be called from inside a Dora XP.&#34;)

    # Convert all to kwargs, add [None] dummy for self which is missing.
    init = Trainer.__init__
    while hasattr(init, &#39;__wrapped__&#39;):
        init = init.__wrapped__
    kwargs = inspect.getcallargs(init, [None] + list(args), **kwargs)
    del kwargs[&#39;self&#39;]

    plugins = kwargs.pop(&#34;plugins&#34;) or []
    env = DoraEnvironment()

    gpus = min(torch.cuda.device_count(), env.world_size())
    if env.world_size() &gt; 1:
        plugins += [env, &#39;dora_ddp&#39;]
    kwargs[&#39;plugins&#39;] = plugins

    callbacks = kwargs.pop(&#34;callbacks&#34;, [])
    callbacks.append(RestoreDoraHistory())
    kwargs[&#39;callbacks&#39;] = callbacks

    if kwargs[&#39;gpus&#39;] is not None:
        raise RuntimeError(&#34;You cannot specify the number of GPUs, as this is provided by Dora.&#34;)
    if kwargs[&#39;num_nodes&#39;] != 1:
        raise RuntimeError(&#34;You cannot specify the number of nodes, as this is provided by Dora.&#34;)

    kwargs[&#39;gpus&#39;] = gpus
    kwargs[&#39;num_nodes&#39;] = env.spec.num_nodes
    kwargs[&#39;default_root_dir&#39;] = get_xp().folder

    if add_dora_logger:
        logger = kwargs[&#39;logger&#39;]
        if logger is True:
            version = os.environ.get(&#39;PL_EXP_VERSION&#39;)
            if version is None:
                version = os.environ.get(&#39;SLURM_JOB_ID&#39;)
            # Create default logger as in PL logger_connector.py
            logger = TensorBoardLogger(
                save_dir=get_xp().folder, version=version, name=&#39;lightning_logs&#39;)
        if not isinstance(logger, tp.Iterable):
            logger = [logger]
        dora_logger = DoraHistoryLogger()
        kwargs[&#39;callbacks&#39;].append(_ArmDoraLogger(dora_logger))
        logger.append(dora_logger)
        kwargs[&#39;logger&#39;] = logger

    trainer = Trainer(**kwargs)

    if no_unfinished_epochs:
        trainer.slurm_connector = _DummySLURMConnector()

    return trainer


class _Intercept:
    @functools.wraps(Trainer.__init__)
    def __init__(self, *args, **kwargs):
        self.args = args
        self.kwargs = kwargs


def trainer_from_argparse_args(args, **kwargs):
    intercept = from_argparse_args(_Intercept, args, **kwargs)
    return get_trainer(*intercept.args, **intercept.kwargs)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dora.lightning.get_trainer"><code class="name flex">
<span>def <span class="ident">get_trainer</span></span>(<span>*args, add_dora_logger=True, no_unfinished_epochs=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a PL trainer, adding the necessary glue code to make everything works.
The arguments are exactly the same as for <code>pytorch_lightning.trainer.Trainer</code>,
with a few extras documented after.</p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;You should not pass <code>gpus=</code> or <code>num_nodes=</code> arguments as those will be filled by Dora.</p>
</div>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>add_dora_logger</code></strong> :&ensp;<code>bool</code></dt>
<dd>if True, adds a Dora Logger to automatically
forward the metrics (those logged with per_epoch=True), otherwise
pushing metrics will be up to you.</dd>
<dt><strong><code>no_unfinished_epochs</code></strong> :&ensp;<code>bool</code></dt>
<dd>if True, deactivates SLURM signal handling
by PL, which can result in half finished epoch with each interruption.
It is recommended to instead dump a checkpoint every epoch and resume
from that one so that training is reliable.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_trainer(*args, add_dora_logger=True, no_unfinished_epochs=True, **kwargs):
    &#34;&#34;&#34;Return a PL trainer, adding the necessary glue code to make everything works.
    The arguments are exactly the same as for `pytorch_lightning.trainer.Trainer`,
    with a few extras documented after.

    ..note:: You should not pass `gpus=` or `num_nodes=` arguments as those will be filled by Dora.

    Args:
        add_dora_logger (bool): if True, adds a Dora Logger to automatically
            forward the metrics (those logged with per_epoch=True), otherwise
            pushing metrics will be up to you.
        no_unfinished_epochs (bool): if True, deactivates SLURM signal handling
            by PL, which can result in half finished epoch with each interruption.
            It is recommended to instead dump a checkpoint every epoch and resume
            from that one so that training is reliable.

    &#34;&#34;&#34;
    if not is_xp():
        raise RuntimeError(&#34;This can only be called from inside a Dora XP.&#34;)

    # Convert all to kwargs, add [None] dummy for self which is missing.
    init = Trainer.__init__
    while hasattr(init, &#39;__wrapped__&#39;):
        init = init.__wrapped__
    kwargs = inspect.getcallargs(init, [None] + list(args), **kwargs)
    del kwargs[&#39;self&#39;]

    plugins = kwargs.pop(&#34;plugins&#34;) or []
    env = DoraEnvironment()

    gpus = min(torch.cuda.device_count(), env.world_size())
    if env.world_size() &gt; 1:
        plugins += [env, &#39;dora_ddp&#39;]
    kwargs[&#39;plugins&#39;] = plugins

    callbacks = kwargs.pop(&#34;callbacks&#34;, [])
    callbacks.append(RestoreDoraHistory())
    kwargs[&#39;callbacks&#39;] = callbacks

    if kwargs[&#39;gpus&#39;] is not None:
        raise RuntimeError(&#34;You cannot specify the number of GPUs, as this is provided by Dora.&#34;)
    if kwargs[&#39;num_nodes&#39;] != 1:
        raise RuntimeError(&#34;You cannot specify the number of nodes, as this is provided by Dora.&#34;)

    kwargs[&#39;gpus&#39;] = gpus
    kwargs[&#39;num_nodes&#39;] = env.spec.num_nodes
    kwargs[&#39;default_root_dir&#39;] = get_xp().folder

    if add_dora_logger:
        logger = kwargs[&#39;logger&#39;]
        if logger is True:
            version = os.environ.get(&#39;PL_EXP_VERSION&#39;)
            if version is None:
                version = os.environ.get(&#39;SLURM_JOB_ID&#39;)
            # Create default logger as in PL logger_connector.py
            logger = TensorBoardLogger(
                save_dir=get_xp().folder, version=version, name=&#39;lightning_logs&#39;)
        if not isinstance(logger, tp.Iterable):
            logger = [logger]
        dora_logger = DoraHistoryLogger()
        kwargs[&#39;callbacks&#39;].append(_ArmDoraLogger(dora_logger))
        logger.append(dora_logger)
        kwargs[&#39;logger&#39;] = logger

    trainer = Trainer(**kwargs)

    if no_unfinished_epochs:
        trainer.slurm_connector = _DummySLURMConnector()

    return trainer</code></pre>
</details>
</dd>
<dt id="dora.lightning.trainer_from_argparse_args"><code class="name flex">
<span>def <span class="ident">trainer_from_argparse_args</span></span>(<span>args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def trainer_from_argparse_args(args, **kwargs):
    intercept = from_argparse_args(_Intercept, args, **kwargs)
    return get_trainer(*intercept.args, **intercept.kwargs)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dora.lightning.DoraDDPPlugin"><code class="flex name class">
<span>class <span class="ident">DoraDDPPlugin</span></span>
<span>(</span><span>parallel_devices: Optional[List[torch.device]] = None, num_nodes: Optional[int] = None, cluster_environment: Optional[pytorch_lightning.plugins.environments.cluster_environment.ClusterEnvironment] = None, checkpoint_io: Optional[pytorch_lightning.plugins.io.checkpoint_plugin.CheckpointIO] = None, sync_batchnorm: Optional[bool] = None, ddp_comm_state: Optional[object] = None, ddp_comm_hook: Optional[<built-in function callable>] = None, ddp_comm_wrapper: Optional[<built-in function callable>] = None, model_averaging_period: Optional[int] = None, **kwargs: Union[Any, Dict[str, Any]])</span>
</code></dt>
<dd>
<div class="desc"><p>DDP plugin for compatibility with Dora.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DoraDDPPlugin(DDPPlugin):
    &#34;&#34;&#34;DDP plugin for compatibility with Dora.
    &#34;&#34;&#34;
    def init_ddp_connection(self, global_rank: tp.Optional[int] = None,
                            world_size: tp.Optional[int] = None) -&gt; None:
        distrib.init(self.torch_distributed_backend)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.plugins.training_type.ddp.DDPPlugin</li>
<li>pytorch_lightning.plugins.training_type.parallel.ParallelPlugin</li>
<li>pytorch_lightning.plugins.training_type.training_type_plugin.TrainingTypePlugin</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dora.lightning.DoraDDPPlugin.init_ddp_connection"><code class="name flex">
<span>def <span class="ident">init_ddp_connection</span></span>(<span>self, global_rank: Optional[int] = None, world_size: Optional[int] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_ddp_connection(self, global_rank: tp.Optional[int] = None,
                        world_size: tp.Optional[int] = None) -&gt; None:
    distrib.init(self.torch_distributed_backend)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dora.lightning.DoraEnvironment"><code class="flex name class">
<span>class <span class="ident">DoraEnvironment</span></span>
</code></dt>
<dd>
<div class="desc"><p>Specification of a cluster environment.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DoraEnvironment(ClusterEnvironment):
    def __init__(self):
        super().__init__()
        self.spec = distrib.get_distrib_spec()

    def creates_children(self) -&gt; bool:
        return True

    def creates_processes_externally(self) -&gt; bool:
        return True

    def master_address(self) -&gt; str:
        return &#34;&#34;

    def master_port(self) -&gt; int:
        assert False

    def world_size(self) -&gt; int:
        return self.spec.world_size

    def set_world_size(self, size: int) -&gt; None:
        pass

    def global_rank(self) -&gt; int:
        return self.spec.rank

    def set_global_rank(self, rank: int) -&gt; None:
        pass

    def local_rank(self) -&gt; int:
        return self.spec.local_rank

    def node_rank(self) -&gt; int:
        return self.spec.node_rank</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.plugins.environments.cluster_environment.ClusterEnvironment</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dora.lightning.DoraEnvironment.creates_children"><code class="name flex">
<span>def <span class="ident">creates_children</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Whether the environment creates the subprocesses or not.</p>
<div class="admonition deprecated">
<p class="admonition-title">Deprecated since version:&ensp;v1.5</p>
<p>This method was deprecated in v1.5 and will be removed in v1.6. Use the property
:attr:<code>creates_processes_externally</code> instead.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def creates_children(self) -&gt; bool:
    return True</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraEnvironment.creates_processes_externally"><code class="name flex">
<span>def <span class="ident">creates_processes_externally</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Whether the environment creates the subprocesses or not.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def creates_processes_externally(self) -&gt; bool:
    return True</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraEnvironment.global_rank"><code class="name flex">
<span>def <span class="ident">global_rank</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>The rank (index) of the currently running process across all nodes and devices.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def global_rank(self) -&gt; int:
    return self.spec.rank</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraEnvironment.local_rank"><code class="name flex">
<span>def <span class="ident">local_rank</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>The rank (index) of the currently running process inside of the current node.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def local_rank(self) -&gt; int:
    return self.spec.local_rank</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraEnvironment.master_address"><code class="name flex">
<span>def <span class="ident">master_address</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>The master address through which all processes connect and communicate.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def master_address(self) -&gt; str:
    return &#34;&#34;</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraEnvironment.master_port"><code class="name flex">
<span>def <span class="ident">master_port</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>An open and configured port in the master node through which all processes communicate.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def master_port(self) -&gt; int:
    assert False</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraEnvironment.node_rank"><code class="name flex">
<span>def <span class="ident">node_rank</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>The rank (index) of the node on which the current process runs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def node_rank(self) -&gt; int:
    return self.spec.node_rank</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraEnvironment.set_global_rank"><code class="name flex">
<span>def <span class="ident">set_global_rank</span></span>(<span>self, rank: int) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_global_rank(self, rank: int) -&gt; None:
    pass</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraEnvironment.set_world_size"><code class="name flex">
<span>def <span class="ident">set_world_size</span></span>(<span>self, size: int) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_world_size(self, size: int) -&gt; None:
    pass</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraEnvironment.world_size"><code class="name flex">
<span>def <span class="ident">world_size</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>The number of processes across all devices and nodes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def world_size(self) -&gt; int:
    return self.spec.world_size</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dora.lightning.DoraHistoryLogger"><code class="flex name class">
<span>class <span class="ident">DoraHistoryLogger</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for experiment loggers.</p>
<h2 id="args">Args</h2>
<p>agg_key_funcs:
Dictionary which maps a metric name to a function, which will
aggregate the metric values for the same steps.
agg_default_func:
Default function to aggregate metric values. If some metric name
is not presented in the <code>agg_key_funcs</code> dictionary, then the
<code>agg_default_func</code> will be used for aggregation.</p>
<h2 id="note">Note</h2>
<p>The <code>agg_key_funcs</code> and <code>agg_default_func</code> arguments are used only when
one logs metrics with the :meth:<code>~LightningLoggerBase.agg_and_log_metrics</code> method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DoraHistoryLogger(LightningLoggerBase):
    def __init__(self):
        super().__init__()
        self.link = get_xp().link
        self.folder = get_xp().folder
        self._metrics = {}

    def log_metrics(self, metrics, step):
        self._metrics.update(metrics)

    def _push(self):
        self.link.push_metrics(self._metrics)
        self._metrics = {}

    def _repush(self):
        history = self.link.history
        history[-1].update(self._metrics)
        self.link.update_history(history)
        self._metrics = {}

    @property
    def save_dir(self):
        return self.folder

    @property
    def name(self):
        return &#34;DoraHistoryLogger&#34;

    def experiment(self) -&gt; tp.Any:
        &#34;&#34;&#34;Return the experiment object associated with this logger.&#34;&#34;&#34;
        pass

    def log_hyperparams(self, params: argparse.Namespace, *args, **kwargs):
        pass

    @property
    def version(self) -&gt; int:
        &#34;&#34;&#34;Return the experiment version.&#34;&#34;&#34;
        return 0</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.loggers.base.LightningLoggerBase</li>
<li>abc.ABC</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="dora.lightning.DoraHistoryLogger.name"><code class="name">var <span class="ident">name</span></code></dt>
<dd>
<div class="desc"><p>Return the experiment name.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self):
    return &#34;DoraHistoryLogger&#34;</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraHistoryLogger.save_dir"><code class="name">var <span class="ident">save_dir</span></code></dt>
<dd>
<div class="desc"><p>Return the root directory where experiment logs get saved, or <code>None</code> if the logger does not save data
locally.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def save_dir(self):
    return self.folder</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraHistoryLogger.version"><code class="name">var <span class="ident">version</span> : int</code></dt>
<dd>
<div class="desc"><p>Return the experiment version.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def version(self) -&gt; int:
    &#34;&#34;&#34;Return the experiment version.&#34;&#34;&#34;
    return 0</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="dora.lightning.DoraHistoryLogger.experiment"><code class="name flex">
<span>def <span class="ident">experiment</span></span>(<span>self) ‑> Any</span>
</code></dt>
<dd>
<div class="desc"><p>Return the experiment object associated with this logger.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def experiment(self) -&gt; tp.Any:
    &#34;&#34;&#34;Return the experiment object associated with this logger.&#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraHistoryLogger.log_hyperparams"><code class="name flex">
<span>def <span class="ident">log_hyperparams</span></span>(<span>self, params: argparse.Namespace, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Record hyperparameters.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong></dt>
<dd>:class:<code>~argparse.Namespace</code> containing the hyperparameters</dd>
<dt><strong><code>args</code></strong></dt>
<dd>Optional positional arguments, depends on the specific logger being used</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Optional keywoard arguments, depends on the specific logger being used</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_hyperparams(self, params: argparse.Namespace, *args, **kwargs):
    pass</code></pre>
</details>
</dd>
<dt id="dora.lightning.DoraHistoryLogger.log_metrics"><code class="name flex">
<span>def <span class="ident">log_metrics</span></span>(<span>self, metrics, step)</span>
</code></dt>
<dd>
<div class="desc"><p>Records metrics.
This method logs metrics as as soon as it received them. If you want to aggregate
metrics for one specific <code>step</code>, use the
:meth:<code>~pytorch_lightning.loggers.base.LightningLoggerBase.agg_and_log_metrics</code> method.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>metrics</code></strong></dt>
<dd>Dictionary with metric names as keys and measured quantities as values</dd>
<dt><strong><code>step</code></strong></dt>
<dd>Step number at which the metrics should be recorded</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_metrics(self, metrics, step):
    self._metrics.update(metrics)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dora.lightning.RestoreDoraHistory"><code class="flex name class">
<span>class <span class="ident">RestoreDoraHistory</span></span>
</code></dt>
<dd>
<div class="desc"><p>Make sure Dora history, and checkpoint state are in sync.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RestoreDoraHistory(Callback):
    &#34;&#34;&#34;Make sure Dora history, and checkpoint state are in sync.
    &#34;&#34;&#34;
    def __init__(self):
        self.link = get_xp().link

    def on_load_checkpoint(self, trainer, pl_module, checkpoint):
        history = checkpoint[&#39;dora_link_history&#39;]
        self.link.update_history(history)

    def on_save_checkpoint(self, trainer, pl_module, checkpoint):
        checkpoint[&#39;dora_link_history&#39;] = self.link.history
        return checkpoint</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.callbacks.base.Callback</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dora.lightning.RestoreDoraHistory.on_load_checkpoint"><code class="name flex">
<span>def <span class="ident">on_load_checkpoint</span></span>(<span>self, trainer, pl_module, checkpoint)</span>
</code></dt>
<dd>
<div class="desc"><p>Called when loading a model checkpoint, use to reload state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trainer</code></strong></dt>
<dd>the current :class:<code>~pytorch_lightning.trainer.Trainer</code> instance.</dd>
<dt><strong><code>pl_module</code></strong></dt>
<dd>the current :class:<code>~pytorch_lightning.core.lightning.LightningModule</code> instance.</dd>
<dt><strong><code>callback_state</code></strong></dt>
<dd>the callback state returned by <code>on_save_checkpoint</code>.</dd>
</dl>
<h2 id="note">Note</h2>
<p>The <code>on_load_checkpoint</code> won't be called with an undefined state.
If your <code>on_load_checkpoint</code> hook behavior doesn't rely on a state,
you will still need to override <code>on_save_checkpoint</code> to return a <code>dummy state</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def on_load_checkpoint(self, trainer, pl_module, checkpoint):
    history = checkpoint[&#39;dora_link_history&#39;]
    self.link.update_history(history)</code></pre>
</details>
</dd>
<dt id="dora.lightning.RestoreDoraHistory.on_save_checkpoint"><code class="name flex">
<span>def <span class="ident">on_save_checkpoint</span></span>(<span>self, trainer, pl_module, checkpoint)</span>
</code></dt>
<dd>
<div class="desc"><p>Called when saving a model checkpoint, use to persist state.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trainer</code></strong></dt>
<dd>the current :class:<code>~pytorch_lightning.trainer.Trainer</code> instance.</dd>
<dt><strong><code>pl_module</code></strong></dt>
<dd>the current :class:<code>~pytorch_lightning.core.lightning.LightningModule</code> instance.</dd>
<dt><strong><code>checkpoint</code></strong></dt>
<dd>the checkpoint dictionary that will be saved.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The callback state.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def on_save_checkpoint(self, trainer, pl_module, checkpoint):
    checkpoint[&#39;dora_link_history&#39;] = self.link.history
    return checkpoint</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dora" href="index.html">dora</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dora.lightning.get_trainer" href="#dora.lightning.get_trainer">get_trainer</a></code></li>
<li><code><a title="dora.lightning.trainer_from_argparse_args" href="#dora.lightning.trainer_from_argparse_args">trainer_from_argparse_args</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dora.lightning.DoraDDPPlugin" href="#dora.lightning.DoraDDPPlugin">DoraDDPPlugin</a></code></h4>
<ul class="">
<li><code><a title="dora.lightning.DoraDDPPlugin.init_ddp_connection" href="#dora.lightning.DoraDDPPlugin.init_ddp_connection">init_ddp_connection</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dora.lightning.DoraEnvironment" href="#dora.lightning.DoraEnvironment">DoraEnvironment</a></code></h4>
<ul class="">
<li><code><a title="dora.lightning.DoraEnvironment.creates_children" href="#dora.lightning.DoraEnvironment.creates_children">creates_children</a></code></li>
<li><code><a title="dora.lightning.DoraEnvironment.creates_processes_externally" href="#dora.lightning.DoraEnvironment.creates_processes_externally">creates_processes_externally</a></code></li>
<li><code><a title="dora.lightning.DoraEnvironment.global_rank" href="#dora.lightning.DoraEnvironment.global_rank">global_rank</a></code></li>
<li><code><a title="dora.lightning.DoraEnvironment.local_rank" href="#dora.lightning.DoraEnvironment.local_rank">local_rank</a></code></li>
<li><code><a title="dora.lightning.DoraEnvironment.master_address" href="#dora.lightning.DoraEnvironment.master_address">master_address</a></code></li>
<li><code><a title="dora.lightning.DoraEnvironment.master_port" href="#dora.lightning.DoraEnvironment.master_port">master_port</a></code></li>
<li><code><a title="dora.lightning.DoraEnvironment.node_rank" href="#dora.lightning.DoraEnvironment.node_rank">node_rank</a></code></li>
<li><code><a title="dora.lightning.DoraEnvironment.set_global_rank" href="#dora.lightning.DoraEnvironment.set_global_rank">set_global_rank</a></code></li>
<li><code><a title="dora.lightning.DoraEnvironment.set_world_size" href="#dora.lightning.DoraEnvironment.set_world_size">set_world_size</a></code></li>
<li><code><a title="dora.lightning.DoraEnvironment.world_size" href="#dora.lightning.DoraEnvironment.world_size">world_size</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dora.lightning.DoraHistoryLogger" href="#dora.lightning.DoraHistoryLogger">DoraHistoryLogger</a></code></h4>
<ul class="two-column">
<li><code><a title="dora.lightning.DoraHistoryLogger.experiment" href="#dora.lightning.DoraHistoryLogger.experiment">experiment</a></code></li>
<li><code><a title="dora.lightning.DoraHistoryLogger.log_hyperparams" href="#dora.lightning.DoraHistoryLogger.log_hyperparams">log_hyperparams</a></code></li>
<li><code><a title="dora.lightning.DoraHistoryLogger.log_metrics" href="#dora.lightning.DoraHistoryLogger.log_metrics">log_metrics</a></code></li>
<li><code><a title="dora.lightning.DoraHistoryLogger.name" href="#dora.lightning.DoraHistoryLogger.name">name</a></code></li>
<li><code><a title="dora.lightning.DoraHistoryLogger.save_dir" href="#dora.lightning.DoraHistoryLogger.save_dir">save_dir</a></code></li>
<li><code><a title="dora.lightning.DoraHistoryLogger.version" href="#dora.lightning.DoraHistoryLogger.version">version</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dora.lightning.RestoreDoraHistory" href="#dora.lightning.RestoreDoraHistory">RestoreDoraHistory</a></code></h4>
<ul class="">
<li><code><a title="dora.lightning.RestoreDoraHistory.on_load_checkpoint" href="#dora.lightning.RestoreDoraHistory.on_load_checkpoint">on_load_checkpoint</a></code></li>
<li><code><a title="dora.lightning.RestoreDoraHistory.on_save_checkpoint" href="#dora.lightning.RestoreDoraHistory.on_save_checkpoint">on_save_checkpoint</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>